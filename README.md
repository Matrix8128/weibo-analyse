3	LDA主题模型简介
本文使用LDA主题模型建立用户的兴趣模型。LDA是一种非监督机器学习技术，可以用来识别文档中的潜藏的主题信息。
LDA模型是一个分层的贝叶斯模型，包含文档、主题和词三个层次。LDA模型的基本思想是每个文档都可以表示成若干潜在主题的多项式分布，每个主题是词汇表中所有单词的多项式分布。
设D为文档集合，共M个文档。Z表示隐含的主题，共K个主题。W为观察到的单词。LDA的两个参数， 表示文档m主题分布， 表示主题k的词汇分布。
LDA对 和 引入了Dirichlet先验分布。Dirichlet分布的公式为：
	 	(3-1)
其中：
	 	(3-2)
根据概率基本性质有   可得
	 	(3-3)
设 和  分别是产生 和  的Dirichlet分布的参数。则LDA模型产生文档可以用下面的概率图模型表示。
 
图3-1A概率图模型
除了 是可观察的变量，其他都是隐含变量。有向边代表存在条件概率依赖，方框代表的是重复次数。M代表语料库中文档的总数。 表示第m个文档所有词汇的主题分布， 表示第n个词的主题， 表示第m个文档的所有词汇， 表示第n个词
根据上图，文档生成的过程可以分解成两个过程：
(1)  ：通过采样以 为参数的Dirichlet分布得到文档-主题分布 ，接着对文档中的每个单词，从多项式分布 中采样得到当前词所在的主题 。
(2)  通过采样以 为参数的Dirichlet分布得到主题-词汇分布 ，接着根据上个过程产生的主题指定 ，从多项分布 中采样当前词 。
根据建立的LDA模型可以写出对应于第m个文档（有 个词）所有变量的联合分布：
 	(3-4)
由于各个文档的生成都是独立的，且 和 与生成过程没有直接相关，则有整个文档集合的似然函数有：
	 	(3-5)
LDA模型的求解就是求出隐含的 和 ，比较流行求解方法有Gibbs Sampling[10]等。
4	面向微博应用的主题模型训练
4.1	语料准备
训练语料主要分成两部分，一部分是复旦大学提供的分类新闻语料，一部分是通过新浪微博API抓取的微博语料。
复旦大学提供的新闻语料包括训练集和测试集，每个大小都在100MB左右。新浪微博的内容质量良莠不齐，充斥着大量的广告，重复无意义的消息等噪音数据，为了保证用于训练的微博语料的质量，本文采集新浪微博系统推荐用户的微博内容作为训练语料。
新浪微博系统推荐共分了20个类别，每个类别有100位推荐用户，这些用户一般都是相关行业著名人物，发布的微博内容质量较高而且有一定的主题倾向，比较适合用于主题模型的训练。获取方法如下：
(1) 先根据api获得全部新浪推荐的用户信息；
(2) 抓取每个用户最近2000条微博（如果不足就获取所有）和标签信息，存入数据库等待处理。
本文中共获取400万条左右微博数据，经过预处理和分词后文本大小有615MB。
4.2	预处理
微博语料在预处理前需要进行去噪音处理，主要包括：
(1) 去除微博短链，表情符号等噪音信息；
(2) 将每个用户获得的所有的微博与标签信息整合成一个用户文档。
新闻语料和微博语料的统一预处理过程：
(1) 去除停用词；
(2) 使用NLPIR分词系统分词[16]；
(3) 根据分词结果中的词性标注进行词性过滤，只保留对主题表达和辨识作用最大的动词和名词[17]。
4.3	模型评价方法
本文使用模型生成测试文档集合W的log似然值 来评价主题模型的性能[15]。设测试文档集合 ， 主题模型参数 。则主题模型生成文档集log似然值 如式(3-1)所示。
	  	（4-1）
其中：
	 	（4-2）
 为第m个文档第n个词， 为第m个文档第n个词的主题， 为第m个文档的单词总数。 
根据似然值 的定义可知， 值越大说明模型生成文档的性能越好，拟合能力越强。
本文主要使用开源工具Mallet进行主题模型的训练，将主题模型应用于微博用户，Mallet[14]是基于java的自然语言处理工具箱，包括了文档分类、聚类、主题模型、信息抽取和其他应用于文本数据的机器学习算法。在主题模型方面，Mallet实现了基本LDA，层次LDA等多个主题模型，而且mallet实现了对模型做log似然评价的方法[15]。
为了得到效果较好的LDA主题模型，本文考虑以下两个方面问题：1）如何确定主题模型的话题数目；2）如何确定模型训练中迭代次数。
4.4	微博话题的数量确定
该实验中，使用5折交叉测试法，迭代1000次，实验结果为测试文档集合W的log似然值 ，不同话题的数量实验结果如表4-1所示。
表4-1 不同话题数量实验结果列表
主题数量	测试结果
60	-100249338
80	-99899792.5
100	-99612941
120	-99391560
140	-99194571.5
200	-98718749
500	-97442256.4
1000	-96518482.5
1500	-96142408.6
2000	-95995661.4
2500	-95992819.8
图4-1给出了不同微博话题数量的主题模型表现结果。可以看出，模型效果随着topic数目的增加而增加，增加速度不断减小，当topic数目到达2000以后，基本不再增加。这与我们的直接感知相符，较多的topic更能精确的反应复杂的现实世界，至于为什么再达到一定数量后上升变得平缓，可能有以下原因：
(1) 语料总量还是太小，体现不出大量的topic
(2) 已接近微博的主要topic数量，再增加也不会有更好的效果

 
图4-1 不同话题数量实验结果图（图中的字号太大！）


4.6	微博话题的迭代次数确定
本文的迭代次数指的是利用Gibbs采样对模型求解过程中的迭代次数。Gibbs采样是一种马尔科夫模特卡罗近似算法，理论上来说，经过足够次数的迭代后马尔科夫链就会达到稳定状态，相应的模型效果也就基本稳定。不过随着迭代次数的增加，模型训练时间、模型训练所需内存和模型大小都会增加，为了在这些指标之间获得平衡，需要寻找合适的迭代次数。
该实验中，测试使用全语料去低频5折交叉测试集（即在原微博5折交叉集合上增加了新闻测试语料），话题数为1000条。实验结果为测试文档集合W的log似然值 ，不同迭代次数的实验结果如表4-2所示。
表4-2 不同迭代次数实验结果列表(这个测试结果可否表示成表4-1的形式？)
迭代次数	测试结果
60	-807453870
80	-807304718
120	-806573027
140	-805389801
200	-804169320
500	-799530173
1000	-794809216
1500	-790092172
2000	-788635105
2500	-786295613
图4-2给出了迭代次数对模型效果的影响。由结果可见，模型效果随迭代次数增加不断增加，增加到2500后依然还有较大的上升势头，但是增长速度已经变缓，虽然马尔科夫链还没有到达稳定状态，不过鉴于再继续增加迭代次数耗时过久，而且迭代过多，模型会变得过大，而当前模型效果已经可用，所以本文就到2500截止
 
图4-2 不同迭代次数实验结果图



图4-3给出了不同迭代次数训练出的模型大小（单位是MB）。可以看出，随着迭代次数的上升，模型大小也在上升，此外相应的训练内存也在上升。其实，当迭代次数达到2500时，生成的模型大小约为1600MB，此时使用模型的时候，导入模型和推断新文档的速度已经很慢。
考虑到迭代2000次生成的模型效果已经不错，速度也可以接受，所以，最终应用的模型迭代次数就是2000次。

 
图4-3 不同迭代次数生成模型大小（图中字号太大！）

5	微博用户兴趣分析
为了验证所获得模型的有效性，本文使用训练得到的主题模型，根据用户文档建立用户兴趣模型并构建了3个相关应用。本文将微博用户元数据，如用户标签、用户转发评论信息、用户关注信息等特征融合进用户文档进行主题分析。
5.1用户数据获取与预处理
本文使用的数据来源包括：
(1) 微博数据；
(2) 用户元数据（标签、个人描述等）；
3）用户单向关注用户数据；
4）用户转发用户数据。
在数据预处理方面，主要完成两方面的工作：
1）去除微博短链、表情、非法字符等噪音数据；
2）提取微博中的微博用户、话题等数据。
在本文的处理过程中，由于各个数据对用户兴趣的表现价值不同，所以需要进行相应的权值调整，以便提升高价值数据（更能体现用户兴趣的数据）在数据中的比例，提升数据质量，以此弥补数据总量较少的缺憾。
例如，用户转发的微博比原创微博更能体现用户兴趣，权值更高用户单向关注用户和被转发的用户都是用户兴趣的直接体现，相应用户数据权值比双向关注用户高。根据调整后的权值，将数据重新组合成一个用户文档。
5.2提取用户兴趣关键词
对于用户兴趣模型，兴趣关键词是个较好的模型表现方式和应用。本文提出的关键词提取实现包括如下几个步骤：
(1) 用户数据经过权值调整后合并成一个用户文档；
(2) 使用改进的NLPIR分词工具对用户文档分词，根据分词结果中的词性标注进行词性过滤，只保留对主题表达作用最大的动词和名词[17]；
3.	使用mallet工具得到单词出现的频次，去除低频词汇；
4.	导入训练得到的主题模型；
5.	根据主题模型得到每个单词的主题分布
在训练得到的主题模型中，每个单词都有一个主题分布概率向量 ，k为模型总的主题数量，  为该单词属于第k个主题的概率。
6.	根据单词的主题分布得到该单词的熵值
VSM模型抽取关键词只用了单词出现的次数来衡量单词的权值，但实际上有一些出现频率很高但属于各个主题的概率比较平均的词汇，这些通常是不具备有主题倾向性的词汇，对用户兴趣的表达基本起不到什么作用。相反，有些单词虽然出现频率不多，但是主题倾向明显，主题分布不均匀，表现在主题分布概率向量上就是属于某些主题的概率极高，属于其他主题的概率极低。而熵值就是用来衡量这种不均匀的标度。单词w的熵H(w)可以用式(5-1)计算得到，熵值越高表示分布的越均匀。
	 	(5-1)

7.	归一化单词频次和单词熵值，按一定比例生成单词权值；
1)	单词w的归一化频次权值MF(w)可用式4-2计算得到：
	 	(5-2)
其中，N为单词总数， 为单词w出现的频次。
2)	单词w的归一化熵值MH(w)可用式5-3计算得到 
	 	(5-3)
其中，N为单词总数， 为单词的熵值。
因为 的计算公式中引入了负号，所以归一化熵值 越高，单词相应的主题分布就越不均匀。
3)	单词w总的权值W(w)是归一化频次权值MF(w)和归一化熵值MH(w)的加权和，则式5-4计算得到： 
	 	(5-4)
其中， ，作为权值分配因子 值越低，归一化熵值MH(w)所占比重越高，经过实验，取 =0.7。
8.	去掉权值过低的单词
经过前7步的计算，运行结果如图5-1所示：
 
图5-1 运行效果图
前面几个提示分别是文档分词、载入模型、获取关键词、有一些关键词在模型中没有出现。
后面输出是一行一个关键词元组，从左到右分别是关键词，频次，归一化熵值，总的权值
9.	单词聚类
使用单词的主题分布概率向量 作为聚类的特征向量，对余下的单词聚类，聚类使用的是weka中实现的EM聚类算法[18]，结果如图5-2所示。
 
图5-2 基于主题模型的聚类效果图
聚类输入是去除过低权值后剩下的395个关键词，聚类给定参数是7，聚成7个类。中间是属于各个类别的关键词，最后是各个类别高权值的关键词具体信息（关键词，频次，归一化熵值，总的权值）。
10.	进一步的关键词提取
根据聚类结果进行，对每个聚类，按单词权值排序取出前50%关键词作为该类代表，打包在前端展示。
图5-3是基于主题模型的关键词提取效果图，图中内容来自本文作者的关键词表，权值越大，字体越大。如：“电影”、，、“视频”等关键词出现频次很高，（电影出现了73次，是频次最高者）；“机器学习”、“算法”、“程序员”等词因为主题倾向性高所以权值也大。
 
图5-3 基于主题模型的关键词提取效果图
5.3用户兴趣相似群体的自动聚类
本部分使用的数据仍然是用于主题模型训练的微博推荐用户语料。虽然初始每个用户都有一个新浪给的推荐类型，不过，作为训练语料，并没有使用该标记，因此，各个微博用户文档是无标注的，而用于用户聚类就是期望从这无标注数据中找出兴趣相似的用户群体。具体过程如算法n所示。
算法n：用户兴趣相似群体的自动聚类
(1) 对用户文档集合进行预处理后，使用训练好的主题模型对每个用户进行主题推断，得到对应的主题分布概率向量  ，k为模型总的主题数量， 为该文档属于第k个主题的概率；
(2) 使用该主题分布概率向量 作为聚类的特征向量，使用weka中的EM算法[18]对用户文档集合聚类，聚类数量取50。
在本文的处理中，每个用户都用“（用户名[类别]）”的形式表示，其中类别就是新浪对该用户的类别标注，不过该标注不能全面地表示用户的兴趣，只能作为参考。图5-4~图5-7是从总体结果中提取出来的一部分聚类示例。
 
图5-4游戏类用户聚类效果
 
图5-5新闻媒体类用户聚类效果
 
图5-7股票，期货类用户聚类效果
 
图5-8体育类用户聚类效果
从以上几张图中可以看出，基本上能把相同兴趣的用户聚在一起（再详细点分析，并举例说明）。

5.4基于用户兴趣模型的用户推荐
根据主题模型得到的用户兴趣主题分布向量，可以用于用户之间兴趣相似度的衡量，继而可以用于给指定用户做有相同兴趣的用户推荐。具体过程如算法n？？所示。

算法n？？：用户推荐
(1) 给定用户，根据提取关键词应用中用到的方法获得用户文档，运用训练得到的主题模型推断出用户的主题分布向量 ；
(2) 同样的方法得到候选推荐用户集合对应的主题分布向量集合；
3.	用主题分布向量的欧几里得距离定义用户兴趣相似度
设两个主题分布向量分布是 , 。他们之间的欧几里得距离定义为：
	  	(5-5)
4.	按兴趣相似度大小排序，给出前n个最相似的用户。
基于用户兴趣模型的用户推荐的效果如图5-9所示。这是为一个摄影师用户推荐的用户，推荐用户后面的数字为距离，距离越小相似度越大。（还能不能再详细分析并举例说明？）

 
图5-9 基于主题模型的用户推荐效果图

